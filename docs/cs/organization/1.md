---
title: 概论
category: 计算机
author: JQiue
---

## 发展简史

### 计算机五代变化

世界上第一台电子数字计算机是1946年在美国宾夕法尼亚大学制成的，这台机器用了18000多个电子管，占地 170m，重达 30 吨，而运算速度只有5000次每秒，站在今天的角度来看这台计算机耗费巨大且又不完善，但是它却是科学史上一次划时代的创新，它奠定了计算机的基础

<div align="center"><img src="https://gitee.com/jqiue/img_upload/raw/master/images/firstcomputer.jpg"/></div>

从使用器件的角度来看，计算机的发展经历了五代的变化

+ 1946~1957 年，**电子管计算机**，集成度小，空间占用大，功耗高，运行速度慢，操作复杂，更换程序需要接线

::: tip
第二次世界大战是电子管计算机产生的催化剂，当时德国的潜艇对英国造成了破坏，英国得知德国海军的密文是通过无线波去发送的，所以英国截获了德国海军的密文，但是问题在于这个密文是通过专门的机器加密的，情报部门不能够直接解密，所以为了解开这个密文，英国就设计了第一台电子管计算机，第一台电子管计算机就是为了破解德国海军密文而产生的
:::

+ 1958~1964 年，**晶体管计算机**，集成度相对电子管较高，空间占用小，功耗相对较低，运行速度较快，操场相对简单，交互更加方便

::: tip
贝尔实验室的三个科学家发明了晶体管
:::

+ 1965~1980 年，**集成电路与大规模集成电路计算机**，计算机变得更小，功耗变得更低，计算速度更快，这个时候计算机具备了进入千家万户的条件，开始有了操作系统的诞生

::: tip
德州仪器的工程师发明了集成电路(IC)
:::

+ 1981年至今，**超大规模集成电路计算机**，一个芯片集成了上百万的晶体管，速度更快，体积更小，价格更低，用途丰富，更能被大众接受

+ 第五个阶段：未来的计算机，以蛋白质分子作为主要原材料的生物计算机，具备体积小，效率高，不易损坏，生物级别的自动修复，以及遵循量子力学的物理计算机

::: tip
2013年5月，谷歌和NASA发布D-Wave Two，2017年5月，中国科学院制造出光量子计算机，2019年1月，IBM展示了世界首款商业化量子计算机，包括一些企业也开始了研究，腾讯在2017年就组建了量子实验室，阿里巴巴在2017年成立了达摩院
:::

### 微型计算机的发展历史

对于微型计算机，主要是从第三个阶段开始的，因为第一个阶段和第二个阶段都不能满足微型计算机的诞生，所以讨论微型计算机是从第三个阶段开始的

在早期，微型计算机使用的都是单核的 CPU

+ 1971 ~ 1973：500KHz频率的微型计算机（字长8位）
+ 1973 ~ 1978：高于1MHz频率的微型计算机（字长8位）
+ 1978 ~ 1985：500MHz频率的微型计算机（字长16位）
+ 1985 ~ 2000：高于1GHz频率的微型计算机（字长32位）
+ 2000到现在：高于2GHz频率的微型计算机（字长64位）

::: tip 摩尔定律
从第三代开始，1956年摩尔观察到芯片上的晶体管每年翻一番，1970年这种姿态减慢成每18年翻一番，这就是摩尔定律，但是到21世纪这个定律开始慢慢的失效了，因为随着时间的发展，CPU 中的集成电路越来越复杂，越来越密集，并且热损耗越来越高，导致 CPU 的性能发展到了瓶颈
:::

为了解决单核 CPU 的性能瓶颈，就从单核 CPU 转向了多核 CPU

+ 2005年 Intel 奔腾系列双核 CPU、AMD速龙系列出现
+ 2006年 Intel 酷睿四核 CPU 推出
+ Intel 酷睿系列十六核 CPU
+ Intel 至强系列五十六核 CPU，并且每一个核心都达到了2GHz以上的频率，算力非常强劲

到了如今讨论微型计算机主要是从 CPU 的发展角度来看待的

## 计算机的分类

计算机分为两大类：

1. 电子模拟计算机：类似于“模拟”的方式，比如时钟用指针在表盘上转动来表示时间，电表用角度来表示电量大小
2. 电子数字计算机：它是用数字来表示数量的大小，主要的特点是按位运算，并且不连续地跳动计算。

模拟计算机由于是通过电压来表示数据，并用盘上连线控制，所以它的精度较低，存储量较小，逻辑判断能力没有，而数字计算机由于是通过数字 0 和 1 表示，并通过程序来控制，所以它的精度非常高，存储量非常大，逻辑判断能力也非常强

电子数字计算机由于应用广泛，所以是现在最主流的计算机，通常也被称为电脑或者电子计算机，并且可以进一步分为**专用计算机**和**通用计算机**，它们是根据计算机的效率、速度、价格、运行的经济性和适应性来划分的

专业计算机是最有效，最经济和最快速的计算机，但是他的适应性很差，通用计算机适应性很强，但是牺牲了效率，速度和经济型，通用计算机又可以分为超级计算机，大型机，服务器，PC 机，单片机和多核机，他们的区别在于体积，简易性，功耗，性能，数据存储容量，指令系统规模和机器价格

<div align="center"><img src="https://gitee.com/jqiue/img_upload/raw/master/images/计算机1.1.png"/></div>

### 超级计算机

功能最强、运算速度最快，存储容量最大的计算机，多用于国家高科技领域和尖端技术研究，比如：天气预报，海洋侦测，科学计算，核聚变，核裂变模拟，以及加密解密算法等等

标记它们运算速度的单位是`TFlop/s`

::: tip
1TFlop/s = 每秒一万一次浮点计算
:::

当前民用级最强 CPU 连超算的十分之一都赶不上，可谓差距之大，对于超级计算机也是有一个算力排名的

名字|制造商|处理器|峰值速度
---|---|---|---
富岳|日本|7299072|513,854.7 Tflops/s
顶点|美国|2414592|200,794.9 Tflops/s
Sierra|美国|1572480|125,712.0 Tflops/s
神威太湖之光|中国|10649600|125,435.9 Tflops/s
天河2号|中国|4981760|100,678.7 Tflops/s

> 资料来源：https://zh.wikipedia.org/wiki/TOP500

### 大型计算机

又称大型机，大型主机等，具有高性能，可处理大量数据与复杂的运算，在大型机市场领域，美国 IBM 公司占据着非常大的市场份额，由于大型机造价非常高昂，国内很多企业都想把大型机给替换掉，也就导致大型机的市场越来越小，提到大型机就不得不提到**去“IOE”化**

::: tip 去 IOE
I 表示 IBM，是服务器主机提供商，O 表示 Oracle，是关系型数据库软件提供商，E 表示 EMC，是存储设备提供商。这三个公司提供的硬件到软件的架构几乎垄断了整个市场，使用它们的产品就代表了高维护费用，仅仅 Oracle 的数据库价格都达到了八位数。随着阿里的用户群越来越大，每年要付出的成本越来越高，阿里在 2008 年提出去IOE化的概念，阿里为了把运维成本降低，增强业务的灵活性，将集中式的 Oracle 数据库改成了分布式的 Mysql 集群，解决了数据库扩展性的问题，并且用普通的服务器代替了大型机。从因果关系来看，去 IOE 行动促成了阿里云的诞生
:::

### 小型机

也叫普通服务器，相比大型机和超级计算机，功耗更低，不需要特殊的空调场所，具备不错的算力，普通服务器已经代替了传统的大型机，称为大规模企业计算的中枢，比如：阿里云，腾讯云，华为云等

### 工作站

工作站是高端的通用微型计算机，提供比个人计算机更强大的性能，类似于普通的台式电脑，体积较大，但性能强劲，一般是给图片或视频渲染工作者来使用的

### 微型计算机

也就是个人计算机，是最普通的一类计算机，可分为台式机，笔记本电脑，一体机，微型计算机虽然看起来成本最低，但是麻雀虽小五脏俱全，从构成的本质来看，个人计算机和其他计算机分类无异，因此只要对个人计算机进行研究，就能触类旁通理解这些计算机的分类

## 计算机的体系与结构

### 冯诺依曼体系

冯诺依曼体系结构即**将程序指令和数据一起存储的计算机设计概念结构**，早期计算机只能运行固定用途的程序，改变程序必须更改结构，重新设计电路，在当时重启程序并不像现在重新编译一样简单，这就导致计算机非常局限，于是冯诺依曼提出了将程序存储起来，并设计通用的电路，运行程序的时候，把程序当成电路能够理解的语言，然后让通用电路执行相关的逻辑，这就是冯诺依曼体系的核心概念：**存储程序指令，设计通用电路**

在冯诺依曼体系中，必须有一个存储器，用于存储程序指令和数据，并且必须有一个控制器，用来控制整个程序的执行流程，必须有一个运算器，用来完成运算相关的操作，还需要有输入设备和输出设备，这五大部件就构成了一个冯诺依曼机，现在的计算机都是冯诺依曼机

+ 输入设备：能够把需要的程序和数据送到计算机中
+ 存储器：能够长期的记忆程序，数据，中间结果以及最终运算的结果
+ 运算器和控制器：具备算术、逻辑运算和数据传送等数据加工处理的能力
+ 输出设备：按照要求将处理的结果输出给用户

<img src="https://gitee.com/jqiue/img_upload/raw/master/images/Snipaste_2020-10-07_18-56-15.png"/>

但是存储器和 CPU 分开会导致一个问题，它们之间的速率问题无法调和，因为 CPU 的处理速度比存储器要快，导致 CPU 经常空转等待数据传输，浪费了 CPU 资源

### 现代计算机结构

现代计算机结构从本质上来讲还是冯诺依曼结构，但是对原有的结构进行了改变来解决冯诺依曼原有结构 CPU 和存储设备之间的性能差异

<img src="https://gitee.com/jqiue/img_upload/raw/master/images/Snipaste_2020-10-07_19-18-47.png"/>

在这个结构中，存储器，运算器，控制器在一个结构上面，现在计算机的结构可以理解为以存储器位核心的结构

::: tip
存储器从广义上来讲指的是磁带，硬盘，U盘，但是这里的存储器指的是更高速的设备：内存和 CPU 寄存器
:::

## 计算机的层次和编程语言

### 程序翻译和程序解释

在计算机中，计算机只能识别 0 和 1 这种二进制语言，而人类的语言和计算机语言是不一样的，需要通过转换成计算机能理解的语言，这个过程就叫翻译或解释

那么翻译和解释有什么区别呢？

假设有一个较为高级的计算机语言 L1，使用 L1 进行程序逻辑的描述，即用 L1 编写程序，这个程序是不能够被计算机执行的，而是生成一个逻辑等价的较为低级的计算机语言 L0，而 L0 是计算机能够实际执行的语言，从 L1 生成 L0 这个过程就叫翻译，用来将 L1 转换成 L0 的工具叫编译器，整个过程就叫程序翻译。而通过 L0 语言实现另外一个程序，这个程序将 L1 语言编写的程序作为输入而执行，每一个 L1 程序的语句都会在 L0 语言实现的程序中进行等价的逻辑转换，这个过程就叫解释，使用 L0 语言实现的另外一个程序就叫解释器，整个过程就叫程序解释。

它们之间的主要区别在于：翻译生成新的程序，解释不生成新的程序，解释由低级语言编写的解释器来解释高级语言编写的程序

代表性的翻译性语言有：C/C++，Object-C，Golang

代表性的解释性语言有：Python，JavaScript

::: tip java和C#是哪种？
从严格意义上来讲，Java既不属于翻译性，也不属于解释性，一个 java 程序执行的时候，首先会被编译成一个 JVM 字节码文件，这个叫做翻译，然后字节码会被解释成字节码，这个过程叫做解释。其中 JVM 虚拟机扮演了一个重要的角色，它是 java 语言特有的一个虚拟机，编写的 java 程序都是在这个虚拟机的上层来执行的
:::

### 计算机的层次与编程语言

如果按层次划分计算机，大致可以划分为七层：

<img src="https://gitee.com/jqiue/img_upload/raw/master/images/Snipaste_2020-10-07_21-48-59.png"/>

+ 硬件逻辑层：由一些门、触发器等逻辑电路组成，属于电子工程的领域，并不是计算机领域
+ 微程序机器层：编程语言主要是一些指令集，由微指令组成的微程序直接交给硬件执行，主要由一些硬件生产部门所编写

::: tip
一条机器指令对应一个微程序，而一个微程序对应着一组微指令
:::

+ 传统机器层：编程语言是 CPU 指令集（机器指令），编写的程序交给微程序直接执行，而指令集则存储在 CPU 内部，对数据的运算进行指导和优化，拥有指令集，CPU 就可以有效地运行，不同的 CPU 架构使用不同的指令集

::: tip 指令集
CPU 制造商只有 Intel 和 AMD，这两个厂商所生产的 CPU 最大的区别就在于它们的指令集不同，但是同一个厂商也可以造出不同指令集的 CPU，比如 x86 和 x86_64 它们之间也是不一样的
:::

+ 操作系统层：向上提供了简易的操作界面，使用户很容易的操作计算机，同时向下对接了指令系统，管理硬件的资源，对用户使用的程序进行资源管理分配
+ 汇编语言层：编程语言即汇编语言，汇编语言可以翻译成可直接执行的机器语言，完成翻译过程的就是汇编器，从这层开始所使用的编程语言开始使用人类比较容易理解的语言
+ 高级语言层：被广大程序员所接受的编程语言，类别非常多，常见的有：C/C++，Java，JavaScript，Python等等，都需要被转换成机器语言执行
+ 应用层：已经不能算是语言了，主要是满足一些计算机针对某种用途而专门设计的应用程序，比如：Office，Adobe等等

## 计算机的计算单位

### 容量单位

容量单位在生活中比较常见，比如：256M的光盘，8G内存，1T硬盘等等，这其中的256M、8G、1T都是容量单位。在计算机的物理层面，它只能认识高低电平，所以就用高低电平来记录信息，通常高电平表示 1 和低电平表示 0，所以计算机只认识0/1这两种状态，而0/1就表示为bit(比特位)，但是0/1表示的内容太少了，为了表示更多的内容，就用8个bit来表示一个字节，在早期计算机数据并不是很大，以字节为单位是当时的主流，但是随着技术的发展，数据的容量也越来越大，这个时候用字节来表示容量已经不够了，因此就推出了更高的容量单位：千字节，兆字节等等

bit|Byte|KB|MB|GB|TB|PB|EB
---|---|---|---|---|---|---|---
比特位|字节|千字节|兆字节|吉字节|太字节|拍字节|艾字节
-|8bits|1024B|1024KB|1024MB|1024GB|1024TB|1024EB
门电路|-|寄存器|高速缓存|内存/硬盘|硬盘|云硬盘|数据仓库

::: warning
在千字节之后，所有的单位的换算关系都是1024，只有字节和比特位使用的是8进制位，同时 1024 = 2^10^
:::

::: danger 为什么网上买的硬盘容量标称500G，格式化之后变成了465G？
这是因为硬盘制造商使用的是10进制位来标记容量，也就是说1G等于1000MB，而不是1024MB，所以(500 * 1000^3^)/1024^3^ 约等于 465，硬盘商使用1000进制位，则是因为硬盘的扇区，也就是存储数据的地方，在记住这个扇区的容量时，使用的是人类容易理解的十进制，更容易去沟通和协商
:::

### 速度单位

#### 网络速度

在互联网走进千家万户的同时，会经常听到各种2M，4M，以及10M这样的关键字，这里的单位并不是存储单位，而是一种速度单位。

::: danger 为什么电信拉的100M光纤，测试峰值速度只有12M每秒？
在网络中常用速度单位为：Mbps，100M 是 100Mbps 的缩写，而 100Mbps 等于 100Mbit/s，所以换算成实际的速度单位为：100Mbit/s = (100/8)MB/s = 12.5MB/s
:::

#### CPU 频率

CPU 的速度一般体现为 CPU 的时钟频率，单位一般是赫兹（Hz）表示，目前主流 CPU 的时钟频率都在 2GHz 以上，了解 CPU 的频率只需要知道赫兹是什么意思

Hz 并不是描述计算机领域所专有的单位，而是用来描述在单位时间内周期性变动重复次数的计算

比如 2GHz 指的是 CPU 每秒钟周期性变化20亿次

## 计算机的字符与编码集

### ASCII码

7个bits就可以完全表示ASCII码，其中包括95个可打印字符，33个不可打印字符（包括控制字符）

::: tip
33 + 95 = 128 = 2^7^
:::

::: warning 缺陷
ASCII的局限在于只能显示26个基本拉丁字母、阿拉伯数字和英式标点符号，因此只能用于显示现代美国英语（且处理naïve、café、élite等外来语时，必须去除附加符号）。虽然EASCII解决了部分西欧语言的显示问题，但对更多其他语言依然无能为力。因此，现在的软件系统大多采用Unicode。
:::

### ASCII码的扩展性

随着时间的发展，ASCII码逐渐的满足不了更多的需求了，主要表现在很多应用或者国家中的符号都无法表示，这时就对ASCII码进行了第一次扩充，将 7bits 增加到 8bits，就可以让ASCII码表示256个字符，于是产生了可扩展的ASCII码

### 字符编码集的国际化

在世界上，很多语言的体系都不一样，很多都是不以有限字符组合的语言，比如英文只用26个英文字母表示，而中文除了偏旁以外，每一个字都是独立的，这些字符尤以中国、韩国、日本等国家的语言最为复杂，所以字符编码集的国际化非常有必要

#### 中文编码集

+ GB2312：又叫《信息交换用汉字编码字符集——基本集》，一共收录了 7445 个字符，包括 6763 个汉字和 682 个其他符号，虽然 GB2312 算是一个比较完备的中文字符集，但是并不符合国际标准
+ GBK：又叫《汉字内码扩展规范》，向下兼容 GB2312，向上支持国际 ISO 标准，收录了 21003 个汉字，支持中日韩全部字符

不管中文编码如何完善，只是一个本地化的编码，跨国使用都会出现乱码现象

### Unicode

又被称为统一码，万国码，单一码，从字面意思来看，Unicode可以表示全世界所有的字符，Unicode 定义了世界通用的符号集，UTF-*实现了编码

比如 UTF-8 以字节为单位对 Unicode 进行编码，编程的时候推荐使用 UTF-8

::: tip
在中国的 Windows 系统是默认使用 GBK 编码，编程时必须注意这个问题
:::

## 计算机的总线

### 总线的概述

在生活中，常用的 USB 就是一种总线，全称“Universal Serial Bus”，译为**通用串行总线**，提供了对外连接的接口，不同的设备都可以通过 USB 接口进行连接，一般鼠标或键盘都是通过 USB 接口来连接计算机的，同时 USB 成为了接口的标准，促使了外围设备接口的统一，减少了使用成本，还有 PCI 总线，常用于主机中的外接显卡

由此看来，总线就是为了解决不同设备之间的通信问题而存在的

计算机中连接五大部件的总线叫做 IO 总线

<div align="center"><img src="https://gitee.com/jqiue/img_upload/raw/master/images/Snipaste_2020-10-11_13-53-03.png"/></div>

### 总线的分类

+ 片内总线：高集成度芯片内部的信息传输线，简化了芯片内部的线路结构，用于连接寄存器，运算器，控制器

+ 系统总线：连接计算机外围设备的信息传输线，用于连接 CPU 主内存，IO 设备等

系统总线也分为三类：数据总线，地址总线，控制总线

1. 数据总线：双向传输各个部件的数据信息，数据总线的位数（总线宽度）是数据总线的重要参数，一般与 CPU 位数相同
2. 地址总线：指定源数据或目的数据在内存中的位置，地址总线的位数与存储单元有关，如果地址总线位数=n，则寻址范围：0~2^n^
3. 控制总线：发出各种控制信号的传输线，可以监视不同部件之间的状态（就绪/未就绪）

### 总线的仲裁

仲裁是用来解决设备之间的使用冲突问题的，比如主存想要和硬盘和其他IO设备交换数据，同时硬盘和其他的IO设备都已经就绪，这个时候就产生了总线是给硬盘使用还是IO设备使用的问题，这就需要仲裁器来解决使用权的问题

#### 仲裁方式

1. 链式查询：电路复杂度低，仲裁方式简单，但是低优先级设备难以获得总线使用权，并且对电路故障敏感
2. 计时器定时查询：对设备进行编号，并使用计数器累积计数，接收到仲裁信号后，往所有设备发出计数值，计数值与设备编号一致则获得总线使用权
3. 独立请求：每个设备均有总线独立连接仲裁器，设备可单独向仲裁器发送请求和接受请求，当接受多个请求信号时，仲裁器有权按优先级分配使用权，响应快，优先顺序可动态改变，设备连线多，总线控制复杂

## 计算机的输入输出设备

### 常见的输入输出设备

+ 字符输入设备

键盘：薄膜键盘，机械键盘，电容键盘

::: tip
机械键盘按照段落感、声音、压力、键程的不同，又被分为：黑轴，红轴，青轴（本人正在使用），和茶轴等，机械键盘操作起来非常带感，敲代码的时候非常奔放
:::

+ 图像输入设备

1. 鼠标
2. 数位板：常用于绘图设计创作，主要包括输入板和压感笔
3. 扫描仪：将图形信息转换为数字信号

+ 图像输出设备

1. 显示器：CRT显示器（大头），液晶显示器
2. 打印机
3. 投影仪

### 输入输出接口的通用设计

对于接口来说，它应该能读取数据，发送数据，检查设备有没有被占用，判断设备是否已经连接，判断设备是否已启动

+ 数据线：是 I/O 设备与主机之间进行数据交换的传送线，根据 I/O 设备的不同，数据线可能是单向，也可能是双向
+ 状态线：I/O 设备状态向主机报告的信号线，主机可查询设备是否已经正常连接并就绪，也可查询设备是否已经被占用
+ 命令线：CPU 向设备发送命令的信号线，比如发送读写信号，启动停止信号等等
+ 设备选择线：主机选择 I/O 设备进行操作的信号线，对连在总线上的设备进行选择

### CPU 与 IO 设备的通信

由于 CPU 速度与 IO 设备的速度不一致，导致通信方式有所不同

+ 程序中断：当外围 IO 设备就绪时，向 CPU 发送中断信号，CPU 内部有专门的电路响应中断信号，是一种提供低速设备通知 CPU 的一种异步方式，CPU 可以高速运转同时兼顾低速设备的响应，但是这样也会降低 CPU 的效率
+ DMA（直接存储器访问）：DMA 直接连接主存与 IO 设备，工作时不需要 CPU 的参与，当主存与 IO 设备交换信息时，不需要中断 CPU，可大大提高 CPU 的效率

## 计算机存储器

### 分类

+ 存储器按照介质分为：半导体存储器和磁存储器

像内存，U盘，固态硬盘都是属于半导体，而磁带和磁盘都是磁存储器

+ 按存储方式分类：随机存储器（RAM）、串行存储器、只读存储器（ROM）

### 存储器的层次结构

读写速度和存储容量都是选择存储器要考虑的因素，根据这些因素就为存储器划分了不同的层次来满足不同的需求

+ 缓存：速度最快，价格最高，存储容量最小，常用于 CPU 内部
+ 主存：速度适中，价格适中，存储容量适中，常用于计算机内部
+ 辅存：速度最慢，价格最低，存储容量最大，常用于外部设备

1. 缓存-主存层次：在 CPU 与主存之间增加一层速度快的 Cache，解决主存速度不足的问题，利用了局部性原理
2. 主存-辅存层次：在主存之外增加辅助存储器，用来解决主存容量不足的问题

::: tip 局部性原理
指 CPU 访问存储器时，无论时存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中，将这个区域换成缓存即可大大提高速度
:::

### 主存储器和辅助存储器

+ RAM（随机存储器：Random Access Memory）：存储的方式是随机的，存储的位置和时间没有关系，通过电容存储数据，必须隔一段时间刷新一次，如果停电，将会丢失所有数据

主存储器由半导体存储器，驱动器，译码器，读写电路，控制电路组成

CPU 中的主存数据寄存器（MDR）通过数据总线和主存中的读写电路进行连接，CPU 中的主存地址寄存器（MAR）通过地址总线和译码器连接

这样 CPU 就可以通过地址总线获得数据位置，同时通过数据总线来传输数据

::: tip
不同位数的操作系统对内存的支持也不同，比如 32 位操作系统只能支持 2^32^ = 4 * 2^30^ = 4GB，即使加更多的内存它最多只能支持4GB，这是因为地址总线的范围最多就是4GB的大小，对于 64 位操作系统来说，内存的支持达到了 2^64^ = 2^34 \* 2^30^ = 2^34^GB
:::

+ HDD（机械硬盘：hard disk drive）：盘片表面是可磁化的特性材料，盘片每分钟几千转，磁头可以移动到指定位置改变电磁的极性方式来进行读写操作

在操作系统中，各个进程可能会不断提出不同对磁盘进行读写操作的请求，因此有必要位磁盘设备建立一个等待队列，因此就产生了调度算法

1. 先来先服务算法：按顺序访问进程的磁道读写需求，公平简单
2. 最短寻道时间优先算法：优先访问距离磁头最近的磁道
3. 扫描算法（电梯调度）：磁头每次只往一个方向移动，到达尽头再反方向移动
4. 循环扫描算法：是对扫描算法的改进，消除了对两端磁道请求的公平

### 高速缓存

CPU 与 cache 之间的数据交换是以字为单位的，而 cache 与主存之间的数据交换是以块为单位的

1. 字：是存放再一个存储单元中的二进制代码组合
2. 字块：存储在连续的存储单元中而被看作是一个单元的一组字

假设一个字有 32 位，一个字块共 B 个字，主存共 M 个字块，那么主存总字数：B \* M，主存总容量：B \* M \* 32

字的地址包含两个部分，前 m 位指定字块的地址，后 b 位指定字在字块中的地址

假设主存用户容量位 4G，字块大小位 4M，字长为 32 位，则对于字地址中的块地址 m 和块内存地址 b 的位数，至少应该是多少

4G = 4096M  
字块数： 4096 / 4 = 1024  
字块地址m：log~2~1024 = 10  
块内字数：4M / 32bit = 1048576  
块内地址b：log~2~1048576 = 20

主存和缓存的逻辑结构类似，缓存的容量较小，缓存的速度更快

CPU 需要的数据在缓存里，就不需要去主存拿，否则需要去主存拿

命中率是衡量缓存的重要性能指标，理论上 CPU 每次都能从告诉缓存中取数据的时候命中率为 1

但这只是理论上的，因为容量不及主存，所以命中率永远不可能为 1

命中率计算方式为：访问 Cache 的次数除以访问主存的次数加访问 Cache 的次数

同时访问效率也是衡量缓存的重要指标

CPU 访问 Cache - 主存的平均时间为：命中率 \* 访问缓存的时间 + (1 - 命中率) \* 访问主存的时间

访问效率为：访问缓存时间 / 平均时间

::: tip 练习
假设 CPU 在执行某段程序时，共访问了 Cache 命中2000次，访问主存50次，已知 Cache 的存取时间为 50ns，主存的存取时间为 200ns，求 Cache - 主存系统的命中率、访问效率、平均访问时间？  
命中率：2000/(2000+50) = 0.97  
CPU 平均访问时间：0.97 \* 50 +(1 - 0.97) \* 200 = 54.5  
访问效率：50 / 54.5 = 0.91 = 91%  
:::

#### 高速缓存的替换策略

对于 CPU 来说，每次能从高速缓存中取到数据无疑能提高效率，这个时候就需要一个性能良好的缓存替换策略，使 CPU 访问缓存中的的数据都是需要的

当 CPU 访问的数据不在缓存中时，就需要从主存中载入数据到缓存中，这就是高速缓存的替换时机，由此产生了不同的算法

+ 随机算法：随机替换掉缓存中的字块
+ 先进先出（FIFO）：看作成一个队列，优先替换最先进入队列的字块
+ 最不经常使用算法（LFU）：优先替换最不经常使用的字块，但是需要额外的空间记录字块的使用频率
+ 最近最少使用算法（LRU）：优先替换一段时间内很少使用的字块，一般使用双向链表实现，把当前访问节点置于链表前面（保证头部节点时最近使用的），替换掉链表尾部字块

## 计算机的指令系统

### 机器指令的形式

机器指令主要由两部分组成：操作码和地址码

+ 操作码指明指令要完成的操作，操作码的位数反映了机器的操作种类，比如 2^8^ 决定了操作码可以完成 256 次操作
+ 地址码用来指出操作数或操作数的地址，使 CPU 找到数据的地址进行相关运算

地址码又分为：三地址指令、二地址指令、一地址指令、零地址指令

+ 三地址指令：有三个地址码，将操作结果放入另外一个地址
+ 二地址指令：有两个地址码，将操作结果放入其中一个地址
+ 一地址指令：有一个地址码，对操作数自身进行操作并覆盖
+ 零地址指令：没有地址码，空操作，停机操作，中断返回操作等不需要操作数的操作

### 机器指令的操作类型

不同的机器指令系统是各不相同的

+ 数据传送：寄存器之间、寄存器与存储单元、存储单元之间的数据传送，数据的读写、交换地址数据、清零置一等操作
+ 算数运算：操作数之间的加减乘除运算，与或非运算，移位运算等
+ 控制指令：等待指令、停机指令、空操作指令、中断指令等

### 机器指令的寻址方式

存储器即可以存放数据，又可以存放指令，其在存储单元中的编号就是地址。

+ 指令寻址

1. 顺序寻址：指令地址在内存中按顺序排列，按一条指令接一条指令的顺序进行
2. 跳跃寻址：指下条指令的地址码不是计数器给的，而是由当前指令给出，跳转到某个位置开始进行

+ 数据寻址

1. 立即寻址：指令直接获得操作数，地址字段指出的不是操作数的地址，而是操作数本身，特点是操作数立即可用，节省了访问内存的时间，但限制了操作数表示范围
2. 直接寻址：直接给出操作数在主存的地址 寻找方式简单，无需计算数据地址，但限制了操作数寻址范围
3. 间接寻址：给出的是操作数地址的地址，需要访问一次或多次主存来获取操作数，操作数寻址范围大，但速度较慢

## 计算机的控制器和运算器

### 控制器

控制器用来协调和控制计算机运行的

+ 程序计数器：用来存储下一条指令的地址，CPU 会不断地循环从计数器中取出指令，当指令被拿出时，指向下一条指令
+ 时序发生器：属于电器工程领域，用于发送时序脉冲，CPU 依据不同的时序脉冲有节奏的进行工作
+ 指令译码器：是最主要的部件，翻译操作码对应的操作以及控制传输地址码对应的数据
+ 各种寄存器
  + 指令寄存器：从主存和告诉缓存取出指令
  + 主存地址寄存器：保存当前 CPU 正要访问的内存单元的地址
  + 主存数据寄存器：保存当前 CPU 正要读或写的主存数据
  + 通用寄存器：暂时存放或传送数据或指令，可以保存 ALU 中间结果，容量也比一般的寄存器要大
+ 总线

### 运算器

运算器用来进行数据处理加工

+ 数据缓冲器：
  + 输入缓冲：暂时存放外设送过来的数据
  + 输出缓冲：暂时存放送往外设的数据
+ ALU：算术逻辑单元是运算器的主要组成部分，可以完成常见的位运算和算术运算
+ 状态字寄存器：存放运算状态（条件码，进位，溢出，结果正负等）和运算控制信息（调试跟踪标记位，允许中断位等）
+ 通用寄存器：暂时存放或传送数据或指令，可以保存 ALU 中间结果，容量也比一般的寄存器要大

### 指令的执行过程

主要有三个过程：取指令、分析指令、执行指令

运算器，控制器，高速缓存都通过片内总线进行通信

从缓存中取到指令后送到指令寄存器，再交给指令译码器进行分析，发出控制信号给运算器，此时程序计数器 + 1，取出下一条指令，运算器会将数据装载到寄存器，通过 ALU 部件处理数据，送出运算结果

#### CPU 的流水线设计

在早期的计算机采用的串行处理，这导致计算机的各个操作只能按照顺序完成，不能够同时进行，为了提高速度，需要一种并行处理的技术
